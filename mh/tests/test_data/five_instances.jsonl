{"repo": "facebookresearch/ParlAI", "pull_number": 2676, "instance_id": "facebookresearch__ParlAI-2676", "issue_numbers": ["2172"], "base_commit": "6c2e66a9f696571fd336ea58365841fd32d15319", "patch": "diff --git a/parlai/core/agents.py b/parlai/core/agents.py\n--- a/parlai/core/agents.py\n+++ b/parlai/core/agents.py\n@@ -43,7 +43,7 @@\n from parlai.core.build_data import modelzoo_path\n from parlai.core.loader import load_agent_module\n from parlai.core.loader import register_agent  # noqa: F401\n-from parlai.core.opt import Opt, load_opt_file\n+from parlai.core.opt import Opt\n from parlai.utils.misc import warn_once\n import copy\n import os\n@@ -202,7 +202,7 @@ def compare_init_model_opts(opt: Opt, curr_opt: Opt):\n     optfile = opt['init_model'] + '.opt'\n     if not os.path.isfile(optfile):\n         return\n-    init_model_opt = load_opt_file(optfile)\n+    init_model_opt = Opt.load(optfile)\n \n     extra_opts = {}\n     different_opts = {}\n@@ -291,7 +291,7 @@ def create_agent_from_opt_file(opt: Opt):\n     model_file = opt['model_file']\n     optfile = model_file + '.opt'\n     if os.path.isfile(optfile):\n-        new_opt = load_opt_file(optfile)\n+        new_opt = Opt.load(optfile)\n         # TODO we need a better way to say these options are never copied...\n         if 'datapath' in new_opt:\n             # never use the datapath from an opt dump\ndiff --git a/parlai/core/opt.py b/parlai/core/opt.py\n--- a/parlai/core/opt.py\n+++ b/parlai/core/opt.py\n@@ -13,6 +13,20 @@\n import pickle\n import traceback\n \n+from typing import List\n+\n+# these keys are automatically removed upon save. This is a rather blunt hammer.\n+# It's preferred you indicate this at option definiton time.\n+__AUTOCLEAN_KEYS__: List[str] = [\n+    \"override\",\n+    \"batchindex\",\n+    \"download_path\",\n+    \"datapath\",\n+    \"batchindex\",\n+    # we don't save interactive mode, it's only decided by scripts or CLI\n+    \"interactive_mode\",\n+]\n+\n \n class Opt(dict):\n     \"\"\"\n@@ -81,17 +95,39 @@ def display_history(self, key):\n         else:\n             return f'No history for {key}'\n \n+    def save(self, filename: str) -> None:\n+        \"\"\"\n+        Save the opt to disk.\n \n-def load_opt_file(optfile: str) -> Opt:\n-    \"\"\"\n-    Load an Opt from disk.\n-    \"\"\"\n-    try:\n-        # try json first\n-        with open(optfile, 'r') as t_handle:\n-            opt = json.load(t_handle)\n-    except UnicodeDecodeError:\n-        # oops it's pickled\n-        with open(optfile, 'rb') as b_handle:\n-            opt = pickle.load(b_handle)\n-    return Opt(opt)\n+        Attempts to 'clean up' any residual values automatically.\n+        \"\"\"\n+        # start with a shallow copy\n+        dct = dict(self)\n+\n+        # clean up some things we probably don't want to save\n+        for key in __AUTOCLEAN_KEYS__:\n+            if key in dct:\n+                del dct[key]\n+\n+        with open(filename, 'w', encoding='utf-8') as f:\n+            json.dump(dct, fp=f, indent=4)\n+            # extra newline for convenience of working with jq\n+            f.write('\\n')\n+\n+    @classmethod\n+    def load(cls, optfile: str) -> 'Opt':\n+        \"\"\"\n+        Load an Opt from disk.\n+        \"\"\"\n+        try:\n+            # try json first\n+            with open(optfile, 'r') as t_handle:\n+                dct = json.load(t_handle)\n+        except UnicodeDecodeError:\n+            # oops it's pickled\n+            with open(optfile, 'rb') as b_handle:\n+                dct = pickle.load(b_handle)\n+        for key in __AUTOCLEAN_KEYS__:\n+            if key in dct:\n+                del dct[key]\n+        return cls(dct)\ndiff --git a/parlai/core/params.py b/parlai/core/params.py\n--- a/parlai/core/params.py\n+++ b/parlai/core/params.py\n@@ -26,7 +26,7 @@\n from parlai.core.build_data import modelzoo_path\n from parlai.core.loader import load_teacher_module, load_agent_module, load_world_module\n from parlai.tasks.tasks import ids_to_tasks\n-from parlai.core.opt import Opt, load_opt_file\n+from parlai.core.opt import Opt\n \n from typing import List, Optional\n \n@@ -123,7 +123,7 @@ def get_model_name(opt):\n             model_file = modelzoo_path(opt.get('datapath'), model_file)\n             optfile = model_file + '.opt'\n             if os.path.isfile(optfile):\n-                new_opt = load_opt_file(optfile)\n+                new_opt = Opt.load(optfile)\n                 model = new_opt.get('model', None)\n     return model\n \n@@ -868,7 +868,7 @@ def _load_known_opts(self, optfile, parsed):\n         Called before args are parsed; ``_load_opts`` is used for actually overriding\n         opts after they are parsed.\n         \"\"\"\n-        new_opt = load_opt_file(optfile)\n+        new_opt = Opt.load(optfile)\n         for key, value in new_opt.items():\n             # existing command line parameters take priority.\n             if key not in parsed or parsed[key] is None:\n@@ -876,7 +876,7 @@ def _load_known_opts(self, optfile, parsed):\n \n     def _load_opts(self, opt):\n         optfile = opt.get('init_opt')\n-        new_opt = load_opt_file(optfile)\n+        new_opt = Opt.load(optfile)\n         for key, value in new_opt.items():\n             # existing command line parameters take priority.\n             if key not in opt:\ndiff --git a/parlai/core/torch_agent.py b/parlai/core/torch_agent.py\n--- a/parlai/core/torch_agent.py\n+++ b/parlai/core/torch_agent.py\n@@ -21,9 +21,7 @@\n \n from typing import Dict, Any, Union, List, Tuple, Optional\n from abc import ABC, abstractmethod\n-from copy import deepcopy\n from collections import deque\n-import json\n import random\n import os\n import torch\n@@ -1788,19 +1786,10 @@ def save(self, path=None):\n             if states:  # anything found to save?\n                 with open(path, 'wb') as write:\n                     torch.save(states, write)\n-\n                 # save opt file\n-                with open(path + '.opt', 'w', encoding='utf-8') as handle:\n-                    if hasattr(self, 'model_version'):\n-                        self.opt['model_version'] = self.model_version()\n-                    saved_opts = deepcopy(self.opt)\n-                    if 'interactive_mode' in saved_opts:\n-                        # We do not save the state of interactive mode, it is only decided\n-                        # by scripts or command line.\n-                        del saved_opts['interactive_mode']\n-                    json.dump(saved_opts, handle, indent=4)\n-                    # for convenience of working with jq, make sure there's a newline\n-                    handle.write('\\n')\n+                if hasattr(self, 'model_version'):\n+                    self.opt['model_version'] = self.model_version()\n+                self.opt.save(path + '.opt')\n \n     def load_state_dict(self, state_dict):\n         \"\"\"\n", "test_patch": "diff --git a/tests/test_opt.py b/tests/test_opt.py\nnew file mode 100644\n--- /dev/null\n+++ b/tests/test_opt.py\n@@ -0,0 +1,33 @@\n+#!/usr/bin/env python3\n+\n+# Copyright (c) Facebook, Inc. and its affiliates.\n+# This source code is licensed under the MIT license found in the\n+# LICENSE file in the root directory of this source tree.\n+\n+import os\n+import unittest\n+import parlai.utils.testing as testing_utils\n+from parlai.core.opt import Opt\n+\n+\"\"\"\n+Test Opt and related mechanisms.\n+\"\"\"\n+\n+\n+class TestOpt(unittest.TestCase):\n+    def test_save_load(self):\n+        o = Opt({'a': 3, 'b': 'foo'})\n+        with testing_utils.tempdir() as tmpdir:\n+            fn = os.path.join(tmpdir, \"opt\")\n+            o.save(fn)\n+            o2 = Opt.load(fn)\n+            assert o == o2\n+\n+    def test_save_withignore(self):\n+        o = Opt({'a': 3, 'b': 'foo', 'override': {'a': 3}})\n+        with testing_utils.tempdir() as tmpdir:\n+            fn = os.path.join(tmpdir, \"opt\")\n+            o.save(fn)\n+            o2 = Opt.load(fn)\n+            assert o != o2\n+            assert 'override' not in o2\n", "problem_statement": "Opt save/load\nBuild a save and load function into opt, and use them. Should help refactor out some nastiness win params.py.\n", "hints_text": "", "created_at": 1589949979000, "PASS_TO_PASS": [], "FAIL_TO_PASS": ["tests/test_opt.py::TestOpt::test_save_load", "tests/test_opt.py::TestOpt::test_save_withignore"], "FAIL_TO_FAIL": [], "difficulty_score": 2, "evaluation_score": 1, "image": "eval-facebookresearch-parlai-9f4529a1", "spec_dict": {"python": "3.9", "pip_packages": ["exceptiongroup==1.2.2", "iniconfig==2.0.0", "packaging==24.2", "pip==25.0.1", "pluggy==1.5.0", "pytest==8.3.5", "pytest-json-report==1.5.0", "pytest-metadata==3.1.1", "setuptools==58.1.0", "tomli==2.2.1", "uv==0.6.6", "wheel==0.45.1"], "test_cmd": "pytest --tb=short --json-report --json-report-file=/pass_report.json -W ignore::DeprecationWarning"}, "task_score": 1, "version": "iid_facebookresearch__ParlAI-2676"}
{"repo": "arviz-devs/arviz", "pull_number": 739, "instance_id": "arviz-devs__arviz-739", "issue_numbers": ["666", "666", "666"], "base_commit": "d1d16fc59848c7e4f9ecfb822a66dab8538731c3", "patch": "diff --git a/arviz/data/inference_data.py b/arviz/data/inference_data.py\n--- a/arviz/data/inference_data.py\n+++ b/arviz/data/inference_data.py\n@@ -42,6 +42,11 @@ def __repr__(self):\n             options=\"\\n\\t> \".join(self._groups)\n         )\n \n+    def __delattr__(self, group):\n+        \"\"\"Delete a group from the InferenceData object.\"\"\"\n+        self._groups.remove(group)\n+        object.__delattr__(self, group)\n+\n     @staticmethod\n     def from_netcdf(filename):\n         \"\"\"Initialize object from a netcdf file.\n@@ -73,16 +78,18 @@ def from_netcdf(filename):\n                     groups[group] = data\n         return InferenceData(**groups)\n \n-    def to_netcdf(self, filename, compress=True):\n+    def to_netcdf(self, filename, compress=True, groups=None):\n         \"\"\"Write InferenceData to file using netcdf4.\n \n         Parameters\n         ----------\n         filename : str\n             Location to write to\n-        compress : bool\n+        compress : bool, optional\n             Whether to compress result. Note this saves disk space, but may make\n             saving and loading somewhat slower (default: True).\n+        groups : list, optional\n+            Write only these groups to netcdf file.\n \n         Returns\n         -------\n@@ -91,7 +98,11 @@ def to_netcdf(self, filename, compress=True):\n         \"\"\"\n         mode = \"w\"  # overwrite first, then append\n         if self._groups:  # check's whether a group is present or not.\n-            for group in self._groups:\n+            if groups is None:\n+                groups = self._groups\n+            else:\n+                groups = [group for group in self._groups if group in groups]\n+            for group in groups:\n                 data = getattr(self, group)\n                 kwargs = {}\n                 if compress:\n", "test_patch": "diff --git a/arviz/tests/test_data.py b/arviz/tests/test_data.py\n--- a/arviz/tests/test_data.py\n+++ b/arviz/tests/test_data.py\n@@ -318,6 +318,45 @@ def test_sel_method(inplace):\n             assert np.all(dataset.draw.values == np.arange(200, ndraws))\r\n \r\n \r\n+@pytest.mark.parametrize(\"use\", (\"del\", \"delattr\"))\r\n+def test_del_method(use):\r\n+    # create inference data object\r\n+    data = np.random.normal(size=(4, 500, 8))\r\n+    idata = from_dict(\r\n+        posterior={\"a\": data[..., 0], \"b\": data},\r\n+        sample_stats={\"a\": data[..., 0], \"b\": data},\r\n+        observed_data={\"b\": data[0, 0, :]},\r\n+        posterior_predictive={\"a\": data[..., 0], \"b\": data},\r\n+    )\r\n+\r\n+    # assert inference data object has all attributes\r\n+    test_dict = {\r\n+        \"posterior\": (\"a\", \"b\"),\r\n+        \"sample_stats\": (\"a\", \"b\"),\r\n+        \"observed_data\": [\"b\"],\r\n+        \"posterior_predictive\": (\"a\", \"b\"),\r\n+    }\r\n+    fails = check_multiple_attrs(test_dict, idata)\r\n+    assert not fails\r\n+    # assert _groups attribute contains all groups\r\n+    groups = getattr(idata, \"_groups\")\r\n+    assert all([group in groups for group in test_dict])\r\n+\r\n+    # Use del method\r\n+    if use == \"del\":\r\n+        del idata.sample_stats\r\n+    else:\r\n+        delattr(idata, \"sample_stats\")\r\n+\r\n+    # assert attribute has been removed\r\n+    test_dict.pop(\"sample_stats\")\r\n+    fails = check_multiple_attrs(test_dict, idata)\r\n+    assert not fails\r\n+    assert not hasattr(idata, \"sample_stats\")\r\n+    # assert _groups attribute has been updated\r\n+    assert \"sample_stats\" not in getattr(idata, \"_groups\")\r\n+\r\n+\r\n class TestNumpyToDataArray:\r\n     def test_1d_dataset(self):\r\n         size = 100\r\n@@ -628,37 +667,79 @@ def get_inference_data(self, data, eight_schools_params):\n         )\r\n \r\n     def test_io_function(self, data, eight_schools_params):\r\n+        # create inference data and assert all attributes are present\r\n         inference_data = self.get_inference_data(  # pylint: disable=W0612\r\n             data, eight_schools_params\r\n         )\r\n-        assert hasattr(inference_data, \"posterior\")\r\n+        test_dict = {\r\n+            \"posterior\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"posterior_predictive\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"sample_stats\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"prior\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"prior_predictive\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"sample_stats_prior\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"observed_data\": [\"J\", \"y\", \"sigma\"],\r\n+        }\r\n+        fails = check_multiple_attrs(test_dict, inference_data)\r\n+        assert not fails\r\n+\r\n+        # check filename does not exist and save InferenceData\r\n         here = os.path.dirname(os.path.abspath(__file__))\r\n         data_directory = os.path.join(here, \"saved_models\")\r\n         filepath = os.path.join(data_directory, \"io_function_testfile.nc\")\r\n         # az -function\r\n         to_netcdf(inference_data, filepath)\r\n+\r\n+        # Assert InferenceData has been saved correctly\r\n         assert os.path.exists(filepath)\r\n         assert os.path.getsize(filepath) > 0\r\n         inference_data2 = from_netcdf(filepath)\r\n-        assert hasattr(inference_data2, \"posterior\")\r\n+        fails = check_multiple_attrs(test_dict, inference_data2)\r\n+        assert not fails\r\n         os.remove(filepath)\r\n         assert not os.path.exists(filepath)\r\n \r\n-    def test_io_method(self, data, eight_schools_params):\r\n+    @pytest.mark.parametrize(\"groups_arg\", [False, True])\r\n+    def test_io_method(self, data, eight_schools_params, groups_arg):\r\n+        # create InferenceData and check it has been properly created\r\n         inference_data = self.get_inference_data(  # pylint: disable=W0612\r\n             data, eight_schools_params\r\n         )\r\n-        assert hasattr(inference_data, \"posterior\")\r\n+        test_dict = {\r\n+            \"posterior\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"posterior_predictive\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"sample_stats\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"prior\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"prior_predictive\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"sample_stats_prior\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+            \"observed_data\": [\"J\", \"y\", \"sigma\"],\r\n+        }\r\n+        fails = check_multiple_attrs(test_dict, inference_data)\r\n+        assert not fails\r\n+\r\n+        # check filename does not exist and use to_netcdf method\r\n         here = os.path.dirname(os.path.abspath(__file__))\r\n         data_directory = os.path.join(here, \"saved_models\")\r\n         filepath = os.path.join(data_directory, \"io_method_testfile.nc\")\r\n         assert not os.path.exists(filepath)\r\n         # InferenceData method\r\n-        inference_data.to_netcdf(filepath)\r\n+        inference_data.to_netcdf(\r\n+            filepath, groups=(\"posterior\", \"observed_data\") if groups_arg else None\r\n+        )\r\n+\r\n+        # assert file has been saved correctly\r\n         assert os.path.exists(filepath)\r\n         assert os.path.getsize(filepath) > 0\r\n         inference_data2 = InferenceData.from_netcdf(filepath)\r\n-        assert hasattr(inference_data2, \"posterior\")\r\n+        if groups_arg:  # if groups arg, update test dict to contain only saved groups\r\n+            test_dict = {\r\n+                \"posterior\": [\"eta\", \"theta\", \"mu\", \"tau\"],\r\n+                \"observed_data\": [\"J\", \"y\", \"sigma\"],\r\n+            }\r\n+            assert not hasattr(inference_data2, \"sample_stats\")\r\n+        fails = check_multiple_attrs(test_dict, inference_data2)\r\n+        assert not fails\r\n+\r\n         os.remove(filepath)\r\n         assert not os.path.exists(filepath)\r\n \r\n", "problem_statement": "Add method to delete groups from InferenceData\n## Tell us about it\r\nAdd a method to delete groups from InferenceData\r\n\r\nWith many groups InferenceData can get quite large, making it hard to add to VCS or email. Things like sample_stats might not be needed for all results sharing. \r\n\r\n\r\n## Thoughts on implementation\r\n1. Delete the object from `az.InferenceData` with `del` method in python\r\n2. Remove string in `az.InferenceData._groups`\nAdd method to delete groups from InferenceData\n## Tell us about it\r\nAdd a method to delete groups from InferenceData\r\n\r\nWith many groups InferenceData can get quite large, making it hard to add to VCS or email. Things like sample_stats might not be needed for all results sharing. \r\n\r\n\r\n## Thoughts on implementation\r\n1. Delete the object from `az.InferenceData` with `del` method in python\r\n2. Remove string in `az.InferenceData._groups`\nAdd method to delete groups from InferenceData\n## Tell us about it\r\nAdd a method to delete groups from InferenceData\r\n\r\nWith many groups InferenceData can get quite large, making it hard to add to VCS or email. Things like sample_stats might not be needed for all results sharing. \r\n\r\n\r\n## Thoughts on implementation\r\n1. Delete the object from `az.InferenceData` with `del` method in python\r\n2. Remove string in `az.InferenceData._groups`\n", "hints_text": "Was this fixed?\nThis is not fixed\nWas this fixed?\nThis is not fixed\nWas this fixed?\nThis is not fixed", "created_at": 1562946306000, "PASS_TO_PASS": ["/workspace/arviz/tests/test_data.py::test_list_datasets", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-True-True-False-chain]", "/workspace/arviz/tests/test_data.py::TestNumpyToDataArray::test_nd_to_inference_data", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-True-False-True-draw]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-False-False-False-draw]", "/workspace/arviz/tests/test_data.py::TestNumpyToDataArray::test_nd_to_dataset", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-False-False-True-chain]", "/workspace/arviz/tests/test_data.py::TestNumpyToDataArray::test_more_chains_than_draws", "/workspace/arviz/tests/test_data.py::TestDataDict::test_inference_data_bad", "/workspace/arviz/tests/test_data.py::test_addition", "/workspace/arviz/tests/test_data.py::test_make_attrs", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-True-True-False-chain]", "/workspace/arviz/tests/test_data.py::test_missing_dataset", "/workspace/arviz/tests/test_data.py::TestDataNetCDF::test_empty_inference_data_object", "/workspace/arviz/tests/test_data.py::test_dims_coords", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-False-False-True-draw]", "/workspace/arviz/tests/test_data.py::test_concat_group[False-True-True]", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-False-True-False-chain]", "/workspace/arviz/tests/test_data.py::test_load_local_arviz_data", "/workspace/arviz/tests/test_data.py::test_concat_edgecases[False-False-False]", "/workspace/arviz/tests/test_data.py::test_convert_to_inference_data_bad", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-True-False-True-chain]", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-False-True-False-draw]", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-False-True-True-chain]", "/workspace/arviz/tests/test_data.py::TestDataConvert::test_convert_to_inference_data", "/workspace/arviz/tests/test_data.py::TestConvertToDataset::test_missing_coords", "/workspace/arviz/tests/test_data.py::test_concat_edgecases[False-False-True]", "/workspace/arviz/tests/test_data.py::test_concat_edgecases[True-False-False]", "/workspace/arviz/tests/test_data.py::test_dict_to_dataset", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-False-False-False-chain]", "/workspace/arviz/tests/test_data.py::test_dims_coords_extra_dims", "/workspace/arviz/tests/test_data.py::test_concat_edgecases[True-False-True]", "/workspace/arviz/tests/test_data.py::test_concat_group[False-True-False]", "/workspace/arviz/tests/test_data.py::TestDataDict::test_inference_data_edge_cases", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-True-True-False-draw]", "/workspace/arviz/tests/test_data.py::test_concat_group[True-True-False]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-False-True-False-chain]", "/workspace/arviz/tests/test_data.py::test_concat_group[True-False-True]", "/workspace/arviz/tests/test_data.py::test_sel_method[True]", "/workspace/arviz/tests/test_data.py::test_concat_bad", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-True-True-True-draw]", "/workspace/arviz/tests/test_data.py::test_convert_to_dataset_idempotent", "/workspace/arviz/tests/test_data.py::TestDataConvert::test_convert_to_dataset", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-False-False-True-draw]", "/workspace/arviz/tests/test_data.py::TestNumpyToDataArray::test_warns_bad_shape", "/workspace/arviz/tests/test_data.py::test_clear_data_home", "/workspace/arviz/tests/test_data.py::TestConvertToDataset::test_skip_dim_0", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-False-True-True-draw]", "/workspace/arviz/tests/test_data.py::TestDataDict::test_inference_data", "/workspace/arviz/tests/test_data.py::test_concat_group[True-True-True]", "/workspace/arviz/tests/test_data.py::TestNumpyToDataArray::test_1d_dataset", "/workspace/arviz/tests/test_data.py::test_convert_to_inference_data_from_file", "/workspace/arviz/tests/test_data.py::test_concat_group[False-False-True]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-True-False-True-chain]", "/workspace/arviz/tests/test_data.py::test_concat_group[True-False-False]", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-False-False-False-draw]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-True-False-False-chain]", "/workspace/arviz/tests/test_data.py::test_concat_edgecases[False-True-True]", "/workspace/arviz/tests/test_data.py::test_concat_edgecases[False-True-False]", "/workspace/arviz/tests/test_data.py::test_bad_checksum", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-False-True-True-chain]", "/workspace/arviz/tests/test_data.py::TestDataDict::test_from_dict_warning", "/workspace/arviz/tests/test_data.py::TestDataNetCDF::test_io_function", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-False-False-True-chain]", "/workspace/arviz/tests/test_data.py::test_bad_inference_data", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-False-True-True-draw]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-True-True-False-draw]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-False-True-False-draw]", "/workspace/arviz/tests/test_data.py::test_concat_edgecases[True-True-True]", "/workspace/arviz/tests/test_data.py::test_convert_to_dataset_bad", "/workspace/arviz/tests/test_data.py::test_load_remote_arviz_data", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-True-False-False-chain]", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-True-True-True-draw]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-True-False-False-draw]", "/workspace/arviz/tests/test_data.py::TestConvertToDataset::test_use_all", "/workspace/arviz/tests/test_data.py::test_sel_method[False]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-True-True-True-chain]", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-True-False-False-draw]", "/workspace/arviz/tests/test_data.py::test_concat_group[False-False-False]", "/workspace/arviz/tests/test_data.py::test_convert_to_inference_data_idempotent", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-False-False-False-chain]", "/workspace/arviz/tests/test_data.py::test_concat_dim[False-True-False-True-draw]", "/workspace/arviz/tests/test_data.py::TestConvertToDataset::test_missing_dims", "/workspace/arviz/tests/test_data.py::test_concat_dim[True-True-True-True-chain]", "/workspace/arviz/tests/test_data.py::test_concat_edgecases[True-True-False]"], "FAIL_TO_PASS": ["/workspace/arviz/tests/test_data.py::TestDataNetCDF::test_io_method[False]", "/workspace/arviz/tests/test_data.py::test_del_method[del]", "/workspace/arviz/tests/test_data.py::TestDataNetCDF::test_io_method[True]", "/workspace/arviz/tests/test_data.py::test_del_method[delattr]"], "FAIL_TO_FAIL": [], "difficulty_score": 2, "evaluation_score": 3, "image": "eval-arviz-devs-arviz-c8efbb14", "spec_dict": {"python": "3.8", "pip_packages": ["arviz==0.4.1", "certifi==2025.1.31", "cftime==1.6.4.post1", "contourpy==1.1.1", "cycler==0.12.1", "exceptiongroup==1.2.2", "fonttools==4.56.0", "importlib-resources==6.4.5", "iniconfig==2.1.0", "kiwisolver==1.4.7", "matplotlib==3.7.5", "netcdf4==1.7.2", "numpy==1.24.4", "packaging==24.2", "pandas==2.0.3", "pillow==10.4.0", "pip==25.0.1", "pluggy==1.5.0", "pyparsing==3.1.4", "pytest==8.3.5", "pytest-json-report==1.5.0", "pytest-metadata==3.1.1", "python-dateutil==2.9.0.post0", "pytz==2025.1", "scipy==1.10.1", "setuptools==57.5.0", "six==1.17.0", "tomli==2.2.1", "tzdata==2025.1", "wheel==0.44.0", "xarray==2023.1.0", "zipp==3.20.2"], "test_cmd": "pytest --continue-on-collection-errors"}, "task_score": 1}
{"repo": "huggingface/datasets", "pull_number": 1982, "instance_id": "huggingface__datasets-1982", "issue_numbers": ["1981"], "base_commit": "21af17b92d59061b30527f8cb1c0d7a67c5694af", "patch": "diff --git a/src/datasets/utils/py_utils.py b/src/datasets/utils/py_utils.py\n--- a/src/datasets/utils/py_utils.py\n+++ b/src/datasets/utils/py_utils.py\n@@ -268,7 +268,7 @@ def flatten_nest_dict(d):\n \n class NestedDataStructure:\n     def __init__(self, data=None):\n-        self.data = data if data else []\n+        self.data = data if data is not None else []\n \n     def flatten(self, data=None):\n         data = data if data is not None else self.data\n", "test_patch": "diff --git a/tests/test_py_utils.py b/tests/test_py_utils.py\n--- a/tests/test_py_utils.py\n+++ b/tests/test_py_utils.py\n@@ -113,6 +113,12 @@ class Foo:\n         self.assertEqual(foo.my_attr, \"bar\")\n \n \n+@pytest.mark.parametrize(\"input_data\", [{}])\n+def test_nested_data_structure_data(input_data):\n+    output_data = NestedDataStructure(input_data).data\n+    assert output_data == input_data\n+\n+\n @pytest.mark.parametrize(\n     \"data, expected_output\",\n     [\n", "problem_statement": "wmt datasets fail to load\non master:\r\n```\r\npython -c 'from datasets import load_dataset; load_dataset(\"wmt14\", \"de-en\")'\r\nDownloading and preparing dataset wmt14/de-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/stas/.cache/huggingface/datasets/wmt14/de-en/1.0.0/43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e...\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 578, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 634, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/stas/.cache/huggingface/modules/datasets_modules/datasets/wmt14/43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e/wmt_utils.py\", line 760, in _split_generators\r\n    extraction_map = dict(downloaded_files, **manual_files)\r\n```\r\n\r\nit worked fine recently. same problem if I try wmt16.\r\n\r\ngit bisect points to this commit from Feb 25 as the culprit https://github.com/huggingface/datasets/commit/792f1d9bb1c5361908f73e2ef7f0181b2be409fa\r\n\r\n@albertvillanova \n", "hints_text": "@stas00 Mea culpa... May I fix this tomorrow morning?\nyes, of course, I reverted to the version before that and it works ;)\r\n\r\nbut since a new release was just made you will probably need to make a hotfix.\r\n\r\nand add the wmt to the tests?\nSure, I will implement a regression test!", "created_at": "2021-03-03T20:16:51Z", "PASS_TO_PASS": ["tests/test_py_utils.py::PyUtilsTest::test_flatten_nest_dict", "tests/test_py_utils.py::test_flatten[data0-expected_output0]", "tests/test_py_utils.py::test_flatten[data14-expected_output14]", "tests/test_py_utils.py::test_flatten[data3-expected_output3]", "tests/test_py_utils.py::PyUtilsTest::test_temporary_assignment", "tests/test_py_utils.py::test_flatten[data9-expected_output9]", "tests/test_py_utils.py::test_flatten[data17-expected_output17]", "tests/test_py_utils.py::PyUtilsTest::test_zip_dict", "tests/test_py_utils.py::test_flatten[data10-expected_output10]", "tests/test_py_utils.py::test_flatten[data16-expected_output16]", "tests/test_py_utils.py::test_flatten[data4-expected_output4]", "tests/test_py_utils.py::test_flatten[data12-expected_output12]", "tests/test_py_utils.py::test_flatten[data5-expected_output5]", "tests/test_py_utils.py::PyUtilsTest::test_map_nested", "tests/test_py_utils.py::test_flatten[data11-expected_output11]", "tests/test_py_utils.py::test_flatten[data8-expected_output8]", "tests/test_py_utils.py::test_flatten[foo-expected_output2]", "tests/test_py_utils.py::test_flatten[data7-expected_output7]", "tests/test_py_utils.py::test_flatten[data6-expected_output6]", "tests/test_py_utils.py::test_flatten[data1-expected_output1]", "tests/test_py_utils.py::test_flatten[data13-expected_output13]", "tests/test_py_utils.py::test_flatten[data15-expected_output15]", "tests/test_py_utils.py::PyUtilsTest::test_zip_nested"], "FAIL_TO_PASS": ["tests/test_py_utils.py::test_nested_data_structure_data[input_data0]"], "FAIL_TO_FAIL": [], "difficulty_score": 1, "evaluation_score": 2, "image": "eval-huggingface-datasets-4dd3eb1c", "spec_dict": {"python": "3.9.21", "install": "pip install --no-build-isolation -e .[tests,metrics-tests]", "test_cmd": "pytest", "pre_install": [], "eval_commands": ["export HF_ALLOW_CODE_EVAL=1"], "additional_notes": "The test command was validated on `tests/test_load.py` with 148 passing tests, 4 failing tests, and 29 warnings. The test failures are likely due to specific issues with the test files or the environment, not the generic test command itself. The warnings are primarily about the need to trust remote code in future releases, which can be addressed by passing the argument `trust_remote_code=True`.", "pip_packages": ["absl-py==2.1.0", "accelerate==1.5.2", "aiobotocore==2.21.1", "aiohappyeyeballs==2.6.1", "aiohttp==3.11.14", "aioitertools==0.12.0", "aiosignal==1.3.2", "annotated-types==0.7.0", "apache-beam==2.63.0", "astunparse==1.6.3", "async-timeout==5.0.1", "attrs==25.3.0", "audioread==3.0.1", "bert-score==0.3.13", "blis==1.2.0", "botocore==1.37.1", "brotli==1.1.0", "catalogue==2.0.10", "certifi==2025.1.31", "cffi==1.17.1", "charset-normalizer==3.4.1", "click==8.1.8", "cloudpathlib==0.21.0", "cloudpickle==2.2.1", "colorama==0.4.6", "confection==0.1.5", "contourpy==1.3.0", "crcmod==1.7", "cycler==0.12.1", "cymem==2.0.11", "decorator==5.2.1", "dill==0.3.1.1", "dnspython==2.7.0", "docopt==0.6.2", "elasticsearch==7.17.12", "exceptiongroup==1.2.2", "execnet==2.1.1", "faiss-cpu==1.10.0", "fastavro==1.10.0", "fasteners==0.19", "filelock==3.18.0", "flatbuffers==25.2.10", "fonttools==4.56.0", "frozenlist==1.5.0", "fsspec==2024.5.0", "gast==0.6.0", "google-pasta==0.2.0", "greenlet==3.1.1", "grpcio==1.65.5", "h5py==3.13.0", "hdfs==2.7.3", "httplib2==0.22.0", "huggingface-hub==0.29.3", "idna==3.10", "importlib-metadata==8.6.1", "importlib-resources==6.5.2", "inflate64==1.0.1", "iniconfig==2.0.0", "jax==0.4.30", "jaxlib==0.4.30", "jinja2==3.1.6", "jiwer==3.1.0", "jmespath==1.0.1", "joblib==1.2.0", "joblibspark==0.5.3", "jsonpickle==3.4.2", "jsonschema==4.23.0", "jsonschema-specifications==2024.10.1", "keras==3.9.0", "kiwisolver==1.4.7", "langcodes==3.5.0", "langdetect==1.0.9", "language-data==1.3.0", "lazy-loader==0.4", "libclang==18.1.1", "librosa==0.11.0", "llvmlite==0.43.0", "lxml==5.3.1", "lz4==4.4.3", "marisa-trie==1.2.1", "markdown==3.7", "markdown-it-py==3.0.0", "markupsafe==3.0.2", "matplotlib==3.9.4", "mauve-text==0.4.0", "mdurl==0.1.2", "ml-dtypes==0.5.1", "mpmath==1.3.0", "msgpack==1.1.0", "multidict==6.2.0", "multiprocess==0.70.9", "multivolumefile==0.2.3", "murmurhash==1.0.12", "namex==0.0.8", "networkx==3.2.1", "nltk==3.9.1", "numba==0.60.0", "numpy==2.0.2", "nvidia-cublas-cu12==12.4.5.8", "nvidia-cuda-cupti-cu12==12.4.127", "nvidia-cuda-nvrtc-cu12==12.4.127", "nvidia-cuda-runtime-cu12==12.4.127", "nvidia-cudnn-cu12==9.1.0.70", "nvidia-cufft-cu12==11.2.1.3", "nvidia-curand-cu12==10.3.5.147", "nvidia-cusolver-cu12==11.6.1.9", "nvidia-cusparse-cu12==12.3.1.170", "nvidia-cusparselt-cu12==0.6.2", "nvidia-nccl-cu12==2.21.5", "nvidia-nvjitlink-cu12==12.4.127", "nvidia-nvtx-cu12==12.4.127", "objsize==0.7.1", "opt-einsum==3.4.0", "optree==0.14.1", "orjson==3.10.15", "packaging==24.2", "pandas==2.2.3", "pillow==11.1.0", "platformdirs==4.3.6", "pluggy==1.5.0", "polars==1.25.2", "pooch==1.8.2", "portalocker==3.1.1", "preshed==3.0.9", "propcache==0.3.0", "proto-plus==1.26.1", "protobuf==3.20.3", "psutil==7.0.0", "py4j==0.10.9.7", "py7zr==0.22.0", "pyarrow==16.1.0", "pyarrow-hotfix==0.6", "pybcj==1.0.3", "pycparser==2.22", "pycryptodomex==3.22.0", "pydantic==2.10.6", "pydantic-core==2.27.2", "pydot==1.4.2", "pygments==2.19.1", "pymongo==4.11.3", "pyparsing==3.2.1", "pyppmd==1.1.1", "pyspark==3.5.5", "pytest==8.3.5", "pytest-datadir==1.6.1", "pytest-xdist==3.6.1", "python-dateutil==2.9.0.post0", "pytz==2025.1", "pyyaml==6.0.2", "pyzstd==0.16.2", "rapidfuzz==3.12.2", "rarfile==4.2", "redis==5.2.1", "referencing==0.36.2", "regex==2024.11.6", "requests==2.32.3", "requests-file==2.1.0", "rich==13.9.4", "rouge-score==0.1.2", "rpds-py==0.23.1", "s3fs==2024.5.0", "sacrebleu==2.5.1", "sacremoses==0.1.1", "safetensors==0.5.3", "scikit-learn==1.6.1", "scipy==1.13.1", "sentencepiece==0.2.0", "seqeval==0.0.10", "setuptools==76.1.0", "six==1.15.0", "smart-open==7.1.0", "sortedcontainers==2.4.0", "soundfile==0.13.1", "soxr==0.5.0.post1", "spacy==3.8.3", "spacy-legacy==3.0.12", "spacy-loggers==1.0.5", "sqlalchemy==2.0.39", "srsly==2.5.1", "sympy==1.13.1", "tabulate==0.9.0", "tensorboard==2.19.0", "tensorboard-data-server==0.7.2", "tensorflow==2.19.0", "tensorflow-io-gcs-filesystem==0.37.1", "termcolor==2.5.0", "texttable==1.7.0", "thinc==8.3.4", "threadpoolctl==3.6.0", "tiktoken==0.9.0", "tldextract==5.1.3", "tokenizers==0.21.1", "toml==0.10.2", "tomli==2.2.1", "torch==2.6.0", "tqdm==4.67.1", "transformers==4.49.0", "triton==3.2.0", "typer==0.4.2", "typing-extensions==4.12.2", "tzdata==2025.1", "urllib3==1.26.20", "wasabi==1.1.3", "weasel==0.4.1", "werkzeug==3.1.3", "wheel==0.45.1", "wrapt==1.17.2", "xxhash==3.5.0", "yarl==1.18.3", "zipp==3.21.0", "zstandard==0.23.0"]}, "task_score": 0}
{"repo": "pypa/pip", "pull_number": 6844, "instance_id": "pypa__pip-6844", "issue_numbers": "['5306']", "base_commit": "c4e45e9b89a4977512983a6ca9b8325ca101522d", "patch": "diff --git a/src/pip/_internal/download.py b/src/pip/_internal/download.py\n--- a/src/pip/_internal/download.py\n+++ b/src/pip/_internal/download.py\n@@ -21,6 +21,7 @@\n from pip._vendor.requests.models import CONTENT_CHUNK_SIZE, Response\n from pip._vendor.requests.structures import CaseInsensitiveDict\n from pip._vendor.requests.utils import get_netrc_auth\n+from pip._vendor.six import PY2\n # NOTE: XMLRPC Client is not annotated in typeshed as on 2017-07-17, which is\n #       why we ignore the type on this import\n from pip._vendor.six.moves import xmlrpc_client  # type: ignore\n@@ -33,7 +34,7 @@\n # Import ssl from compat so the initial import occurs in only one place.\n from pip._internal.utils.compat import HAS_TLS, ssl\n from pip._internal.utils.encoding import auto_decode\n-from pip._internal.utils.filesystem import check_path_owner\n+from pip._internal.utils.filesystem import check_path_owner, copy2_fixed\n from pip._internal.utils.glibc import libc_ver\n from pip._internal.utils.marker_files import write_delete_marker_file\n from pip._internal.utils.misc import (\n@@ -49,6 +50,7 @@\n     format_size,\n     get_installed_version,\n     netloc_has_port,\n+    path_to_display,\n     path_to_url,\n     remove_auth_from_url,\n     rmtree,\n@@ -63,15 +65,39 @@\n \n if MYPY_CHECK_RUNNING:\n     from typing import (\n-        Optional, Tuple, Dict, IO, Text, Union\n+        Callable, Dict, List, IO, Optional, Text, Tuple, Union\n     )\n     from optparse import Values\n+\n+    from mypy_extensions import TypedDict\n+\n     from pip._internal.models.link import Link\n     from pip._internal.utils.hashes import Hashes\n     from pip._internal.vcs.versioncontrol import AuthInfo, VersionControl\n \n     Credentials = Tuple[str, str, str]\n \n+    if PY2:\n+        CopytreeKwargs = TypedDict(\n+            'CopytreeKwargs',\n+            {\n+                'ignore': Callable[[str, List[str]], List[str]],\n+                'symlinks': bool,\n+            },\n+            total=False,\n+        )\n+    else:\n+        CopytreeKwargs = TypedDict(\n+            'CopytreeKwargs',\n+            {\n+                'copy_function': Callable[[str, str], None],\n+                'ignore': Callable[[str, List[str]], List[str]],\n+                'ignore_dangling_symlinks': bool,\n+                'symlinks': bool,\n+            },\n+            total=False,\n+        )\n+\n \n __all__ = ['get_file_content',\n            'is_url', 'url_to_path', 'path_to_url',\n@@ -944,6 +970,46 @@ def unpack_http_url(\n             os.unlink(from_path)\n \n \n+def _copy2_ignoring_special_files(src, dest):\n+    # type: (str, str) -> None\n+    \"\"\"Copying special files is not supported, but as a convenience to users\n+    we skip errors copying them. This supports tools that may create e.g.\n+    socket files in the project source directory.\n+    \"\"\"\n+    try:\n+        copy2_fixed(src, dest)\n+    except shutil.SpecialFileError as e:\n+        # SpecialFileError may be raised due to either the source or\n+        # destination. If the destination was the cause then we would actually\n+        # care, but since the destination directory is deleted prior to\n+        # copy we ignore all of them assuming it is caused by the source.\n+        logger.warning(\n+            \"Ignoring special file error '%s' encountered copying %s to %s.\",\n+            str(e),\n+            path_to_display(src),\n+            path_to_display(dest),\n+        )\n+\n+\n+def _copy_source_tree(source, target):\n+    # type: (str, str) -> None\n+    def ignore(d, names):\n+        # Pulling in those directories can potentially be very slow,\n+        # exclude the following directories if they appear in the top\n+        # level dir (and only it).\n+        # See discussion at https://github.com/pypa/pip/pull/6770\n+        return ['.tox', '.nox'] if d == source else []\n+\n+    kwargs = dict(ignore=ignore, symlinks=True)  # type: CopytreeKwargs\n+\n+    if not PY2:\n+        # Python 2 does not support copy_function, so we only ignore\n+        # errors on special file copy in Python 3.\n+        kwargs['copy_function'] = _copy2_ignoring_special_files\n+\n+    shutil.copytree(source, target, **kwargs)\n+\n+\n def unpack_file_url(\n     link,  # type: Link\n     location,  # type: str\n@@ -959,21 +1025,9 @@ def unpack_file_url(\n     link_path = url_to_path(link.url_without_fragment)\n     # If it's a url to a local directory\n     if is_dir_url(link):\n-\n-        def ignore(d, names):\n-            # Pulling in those directories can potentially be very slow,\n-            # exclude the following directories if they appear in the top\n-            # level dir (and only it).\n-            # See discussion at https://github.com/pypa/pip/pull/6770\n-            return ['.tox', '.nox'] if d == link_path else []\n-\n         if os.path.isdir(location):\n             rmtree(location)\n-        shutil.copytree(link_path,\n-                        location,\n-                        symlinks=True,\n-                        ignore=ignore)\n-\n+        _copy_source_tree(link_path, location)\n         if download_dir:\n             logger.info('Link is a directory, ignoring download_dir')\n         return\ndiff --git a/src/pip/_internal/utils/filesystem.py b/src/pip/_internal/utils/filesystem.py\n--- a/src/pip/_internal/utils/filesystem.py\n+++ b/src/pip/_internal/utils/filesystem.py\n@@ -1,5 +1,7 @@\n import os\n import os.path\n+import shutil\n+import stat\n \n from pip._internal.utils.compat import get_path_uid\n \n@@ -28,3 +30,32 @@ def check_path_owner(path):\n         else:\n             previous, path = path, os.path.dirname(path)\n     return False  # assume we don't own the path\n+\n+\n+def copy2_fixed(src, dest):\n+    # type: (str, str) -> None\n+    \"\"\"Wrap shutil.copy2() but map errors copying socket files to\n+    SpecialFileError as expected.\n+\n+    See also https://bugs.python.org/issue37700.\n+    \"\"\"\n+    try:\n+        shutil.copy2(src, dest)\n+    except (OSError, IOError):\n+        for f in [src, dest]:\n+            try:\n+                is_socket_file = is_socket(f)\n+            except OSError:\n+                # An error has already occurred. Another error here is not\n+                # a problem and we can ignore it.\n+                pass\n+            else:\n+                if is_socket_file:\n+                    raise shutil.SpecialFileError(\"`%s` is a socket\" % f)\n+\n+        raise\n+\n+\n+def is_socket(path):\n+    # type: (str) -> bool\n+    return stat.S_ISSOCK(os.lstat(path).st_mode)\n", "test_patch": "diff --git a/tests/functional/test_install.py b/tests/functional/test_install.py\n--- a/tests/functional/test_install.py\n+++ b/tests/functional/test_install.py\n@@ -1,6 +1,7 @@\n import distutils\n import glob\n import os\n+import shutil\n import sys\n import textwrap\n from os.path import curdir, join, pardir\n@@ -23,6 +24,7 @@\n     pyversion_tuple,\n     requirements_file,\n )\n+from tests.lib.filesystem import make_socket_file\n from tests.lib.local_repos import local_checkout\n from tests.lib.path import Path\n \n@@ -488,6 +490,29 @@ def test_install_from_local_directory_with_symlinks_to_directories(\n     assert egg_info_folder in result.files_created, str(result)\n \n \n+@pytest.mark.skipif(\"sys.platform == 'win32' or sys.version_info < (3,)\")\n+def test_install_from_local_directory_with_socket_file(script, data, tmpdir):\n+    \"\"\"\n+    Test installing from a local directory containing a socket file.\n+    \"\"\"\n+    egg_info_file = (\n+        script.site_packages / \"FSPkg-0.1.dev0-py%s.egg-info\" % pyversion\n+    )\n+    package_folder = script.site_packages / \"fspkg\"\n+    to_copy = data.packages.joinpath(\"FSPkg\")\n+    to_install = tmpdir.joinpath(\"src\")\n+\n+    shutil.copytree(to_copy, to_install)\n+    # Socket file, should be ignored.\n+    socket_file_path = os.path.join(to_install, \"example\")\n+    make_socket_file(socket_file_path)\n+\n+    result = script.pip(\"install\", \"--verbose\", to_install, expect_error=False)\n+    assert package_folder in result.files_created, str(result.stdout)\n+    assert egg_info_file in result.files_created, str(result)\n+    assert str(socket_file_path) in result.stderr\n+\n+\n def test_install_from_local_directory_with_no_setup_py(script, data):\n     \"\"\"\n     Test installing from a local directory with no 'setup.py'.\ndiff --git a/tests/lib/filesystem.py b/tests/lib/filesystem.py\nnew file mode 100644\n--- /dev/null\n+++ b/tests/lib/filesystem.py\n@@ -0,0 +1,48 @@\n+\"\"\"Helpers for filesystem-dependent tests.\n+\"\"\"\n+import os\n+import socket\n+import subprocess\n+import sys\n+from functools import partial\n+from itertools import chain\n+\n+from .path import Path\n+\n+\n+def make_socket_file(path):\n+    # Socket paths are limited to 108 characters (sometimes less) so we\n+    # chdir before creating it and use a relative path name.\n+    cwd = os.getcwd()\n+    os.chdir(os.path.dirname(path))\n+    try:\n+        sock = socket.socket(socket.AF_UNIX)\n+        sock.bind(os.path.basename(path))\n+    finally:\n+        os.chdir(cwd)\n+\n+\n+def make_unreadable_file(path):\n+    Path(path).touch()\n+    os.chmod(path, 0o000)\n+    if sys.platform == \"win32\":\n+        # Once we drop PY2 we can use `os.getlogin()` instead.\n+        username = os.environ[\"USERNAME\"]\n+        # Remove \"Read Data/List Directory\" permission for current user, but\n+        # leave everything else.\n+        args = [\"icacls\", path, \"/deny\", username + \":(RD)\"]\n+        subprocess.check_call(args)\n+\n+\n+def get_filelist(base):\n+    def join(dirpath, dirnames, filenames):\n+        relative_dirpath = os.path.relpath(dirpath, base)\n+        join_dirpath = partial(os.path.join, relative_dirpath)\n+        return chain(\n+            (join_dirpath(p) for p in dirnames),\n+            (join_dirpath(p) for p in filenames),\n+        )\n+\n+    return set(chain.from_iterable(\n+        join(*dirinfo) for dirinfo in os.walk(base)\n+    ))\ndiff --git a/tests/unit/test_download.py b/tests/unit/test_download.py\n--- a/tests/unit/test_download.py\n+++ b/tests/unit/test_download.py\n@@ -1,6 +1,7 @@\n import functools\n import hashlib\n import os\n+import shutil\n import sys\n from io import BytesIO\n from shutil import copy, rmtree\n@@ -15,6 +16,7 @@\n     MultiDomainBasicAuth,\n     PipSession,\n     SafeFileCache,\n+    _copy_source_tree,\n     _download_http_url,\n     _get_url_scheme,\n     parse_content_disposition,\n@@ -28,6 +30,12 @@\n from pip._internal.utils.hashes import Hashes\n from pip._internal.utils.misc import path_to_url\n from tests.lib import create_file\n+from tests.lib.filesystem import (\n+    get_filelist,\n+    make_socket_file,\n+    make_unreadable_file,\n+)\n+from tests.lib.path import Path\n \n \n @pytest.fixture(scope=\"function\")\n@@ -334,6 +342,85 @@ def test_url_to_path_path_to_url_symmetry_win():\n     assert url_to_path(path_to_url(unc_path)) == unc_path\n \n \n+@pytest.fixture\n+def clean_project(tmpdir_factory, data):\n+    tmpdir = Path(str(tmpdir_factory.mktemp(\"clean_project\")))\n+    new_project_dir = tmpdir.joinpath(\"FSPkg\")\n+    path = data.packages.joinpath(\"FSPkg\")\n+    shutil.copytree(path, new_project_dir)\n+    return new_project_dir\n+\n+\n+def test_copy_source_tree(clean_project, tmpdir):\n+    target = tmpdir.joinpath(\"target\")\n+    expected_files = get_filelist(clean_project)\n+    assert len(expected_files) == 3\n+\n+    _copy_source_tree(clean_project, target)\n+\n+    copied_files = get_filelist(target)\n+    assert expected_files == copied_files\n+\n+\n+@pytest.mark.skipif(\"sys.platform == 'win32' or sys.version_info < (3,)\")\n+def test_copy_source_tree_with_socket(clean_project, tmpdir, caplog):\n+    target = tmpdir.joinpath(\"target\")\n+    expected_files = get_filelist(clean_project)\n+    socket_path = str(clean_project.joinpath(\"aaa\"))\n+    make_socket_file(socket_path)\n+\n+    _copy_source_tree(clean_project, target)\n+\n+    copied_files = get_filelist(target)\n+    assert expected_files == copied_files\n+\n+    # Warning should have been logged.\n+    assert len(caplog.records) == 1\n+    record = caplog.records[0]\n+    assert record.levelname == 'WARNING'\n+    assert socket_path in record.message\n+\n+\n+@pytest.mark.skipif(\"sys.platform == 'win32' or sys.version_info < (3,)\")\n+def test_copy_source_tree_with_socket_fails_with_no_socket_error(\n+    clean_project, tmpdir\n+):\n+    target = tmpdir.joinpath(\"target\")\n+    expected_files = get_filelist(clean_project)\n+    make_socket_file(clean_project.joinpath(\"aaa\"))\n+    unreadable_file = clean_project.joinpath(\"bbb\")\n+    make_unreadable_file(unreadable_file)\n+\n+    with pytest.raises(shutil.Error) as e:\n+        _copy_source_tree(clean_project, target)\n+\n+    errored_files = [err[0] for err in e.value.args[0]]\n+    assert len(errored_files) == 1\n+    assert unreadable_file in errored_files\n+\n+    copied_files = get_filelist(target)\n+    # All files without errors should have been copied.\n+    assert expected_files == copied_files\n+\n+\n+def test_copy_source_tree_with_unreadable_dir_fails(clean_project, tmpdir):\n+    target = tmpdir.joinpath(\"target\")\n+    expected_files = get_filelist(clean_project)\n+    unreadable_file = clean_project.joinpath(\"bbb\")\n+    make_unreadable_file(unreadable_file)\n+\n+    with pytest.raises(shutil.Error) as e:\n+        _copy_source_tree(clean_project, target)\n+\n+    errored_files = [err[0] for err in e.value.args[0]]\n+    assert len(errored_files) == 1\n+    assert unreadable_file in errored_files\n+\n+    copied_files = get_filelist(target)\n+    # All files without errors should have been copied.\n+    assert expected_files == copied_files\n+\n+\n class Test_unpack_file_url(object):\n \n     def prep(self, tmpdir, data):\ndiff --git a/tests/unit/test_utils_filesystem.py b/tests/unit/test_utils_filesystem.py\nnew file mode 100644\n--- /dev/null\n+++ b/tests/unit/test_utils_filesystem.py\n@@ -0,0 +1,61 @@\n+import os\n+import shutil\n+\n+import pytest\n+\n+from pip._internal.utils.filesystem import copy2_fixed, is_socket\n+from tests.lib.filesystem import make_socket_file, make_unreadable_file\n+from tests.lib.path import Path\n+\n+\n+def make_file(path):\n+    Path(path).touch()\n+\n+\n+def make_valid_symlink(path):\n+    target = path + \"1\"\n+    make_file(target)\n+    os.symlink(target, path)\n+\n+\n+def make_broken_symlink(path):\n+    os.symlink(\"foo\", path)\n+\n+\n+def make_dir(path):\n+    os.mkdir(path)\n+\n+\n+skip_on_windows = pytest.mark.skipif(\"sys.platform == 'win32'\")\n+\n+\n+@skip_on_windows\n+@pytest.mark.parametrize(\"create,result\", [\n+    (make_socket_file, True),\n+    (make_file, False),\n+    (make_valid_symlink, False),\n+    (make_broken_symlink, False),\n+    (make_dir, False),\n+])\n+def test_is_socket(create, result, tmpdir):\n+    target = tmpdir.joinpath(\"target\")\n+    create(target)\n+    assert os.path.lexists(target)\n+    assert is_socket(target) == result\n+\n+\n+@pytest.mark.parametrize(\"create,error_type\", [\n+    pytest.param(\n+        make_socket_file, shutil.SpecialFileError, marks=skip_on_windows\n+    ),\n+    (make_unreadable_file, OSError),\n+])\n+def test_copy2_fixed_raises_appropriate_errors(create, error_type, tmpdir):\n+    src = tmpdir.joinpath(\"src\")\n+    create(src)\n+    dest = tmpdir.joinpath(\"dest\")\n+\n+    with pytest.raises(error_type):\n+        copy2_fixed(src, dest)\n+\n+    assert not dest.exists()\n", "problem_statement": "pip install -U . fails when there's a UNIX domain socket in the current directory\n* Pip version: 10.0.1\r\n* Python version: 3.6\r\n* Operating system: macOS Sierra\r\n\r\n### Description:\r\n\r\nIt seems pip 10.0.1, when installing from a directory, makes a copy of that directory before doing anything else (?). This causes problems for mypy developers. We have a daemon process that creates a UNIX domain socket named `dmypy.sock` in the current directory. When the mypy daemon is running in the mypy directory, and we try to install from there, this copy fails with the following error:\r\n```\r\n$ pip3 install -U .\r\nProcessing /Users/guido/src/mypy\r\nCould not install packages due to an EnvironmentError: [('/Users/guido/src/mypy/dmypy.sock', '/private/var/folders/63/czkyq6090dd0t157zhx54xvhrdlybt/T/pip-req-build-yucol_r3/dmypy.sock', \"[Errno 102] Operation not supported on socket: '/Users/guido/src/mypy/dmypy.sock'\")]\r\n```\r\nUsing `-v` we get the following traceback:\r\n```\r\n$ pip3 install -U . -v\r\nCreated temporary directory: /private/var/folders/63/czkyq6090dd0t157zhx54xvhrdlybt/T/pip-ephem-wheel-cache-z05l_dty\r\nCreated temporary directory: /private/var/folders/63/czkyq6090dd0t157zhx54xvhrdlybt/T/pip-install-pdgnid0m\r\nProcessing /Users/guido/src/mypy\r\n  Created temporary directory: /private/var/folders/63/czkyq6090dd0t157zhx54xvhrdlybt/T/pip-req-build-cykzr90d\r\nCould not install packages due to an EnvironmentError.\r\nTraceback (most recent call last):\r\n  File \"/Users/guido/v36/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 291, in run\r\n    resolver.resolve(requirement_set)\r\n  File \"/Users/guido/v36/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 103, in resolve\r\n    self._resolve_one(requirement_set, req)\r\n  File \"/Users/guido/v36/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 257, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"/Users/guido/v36/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 210, in _get_abstract_dist_for\r\n    self.require_hashes\r\n  File \"/Users/guido/v36/lib/python3.6/site-packages/pip/_internal/operations/prepare.py\", line 310, in prepare_linked_requirement\r\n    progress_bar=self.progress_bar\r\n  File \"/Users/guido/v36/lib/python3.6/site-packages/pip/_internal/download.py\", line 824, in unpack_url\r\n    unpack_file_url(link, location, download_dir, hashes=hashes)\r\n  File \"/Users/guido/v36/lib/python3.6/site-packages/pip/_internal/download.py\", line 700, in unpack_file_url\r\n    shutil.copytree(link_path, location, symlinks=True)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/shutil.py\", line 359, in copytree\r\n    raise Error(errors)\r\nshutil.Error: [('/Users/guido/src/mypy/dmypy.sock', '/private/var/folders/63/czkyq6090dd0t157zhx54xvhrdlybt/T/pip-req-build-cykzr90d/dmypy.sock', \"[Errno 102] Operation not supported on socket: '/Users/guido/src/mypy/dmypy.sock'\")]\r\nCleaning up...\r\n```\r\nA simple workaround is to stop the daemon (or if it's been killed, `rm dmypy.sock`).\r\n\r\n(First reported for mypy: https://github.com/python/mypy/issues/4945, for @ilevkivsky.)\n", "hints_text": "A fix for this would be that pip tries to be smarter about what it's copying.\r\n\r\nHonestly though, I have a feeling that adding support for a `.pipignore` will let people work around edge cases and that would be cleaner than adding support for all kinds of edge cases in pip.\r\n\r\nUnrelated: pip will always reinstall from a local directory -- `pip install .` should just work for you. :)\r\n\nIt's also arguably a bug in `shutil.copytree()`, which contains [this comment](https://github.com/python/cpython/blob/b81ca28b378c8b29204a37f8bd433a3379f53f7d/Lib/shutil.py#L113):\r\n```\r\n# XXX What about other special files? (sockets, devices...)\r\n```\r\n\n(And thanks for the tip about not needing `-U`! I learned something. :-)\nFor some background see issue #2195 \u2013 *pip install of a directory is super slow*.\r\nProblem with source directory containing sockets was raised in issue #2974 \u2013 *Wheel command does not work if the directory contains a socket*.\r\n", "created_at": "2019-08-08T00:00:15Z", "PASS_TO_PASS": [], "FAIL_TO_PASS": ["tests/unit/test_download.py::test_sanitize_content_filename__platform_dependent[..\\\\--..\\\\]", "tests/unit/test_download.py::test_url_to_path[file://localhost/c:/tmp/file-C:\\\\tmp\\\\file-/c:/tmp/file]", "tests/unit/test_download.py::test_sanitize_content_filename[/-]", "tests/unit/test_utils_filesystem.py::test_copy2_fixed_raises_appropriate_errors[make_socket_file-SpecialFileError]", "tests/unit/test_download.py::test_sanitize_content_filename[dir/file-file]", "tests/unit/test_download.py::test_user_agent__ci[BUILD_ID-True]", "tests/unit/test_utils_filesystem.py::test_is_socket[make_valid_symlink-False]", "tests/unit/test_download.py::test_sanitize_content_filename__platform_dependent[\\\\--\\\\]", "tests/unit/test_download.py::test_sanitize_content_filename__platform_dependent[..\\\\..\\\\file-file-..\\\\..\\\\file]", "tests/unit/test_download.py::test_url_to_path[file:c:/path/to/file-C:\\\\path\\\\to\\\\file-c:/path/to/file]", "tests/unit/test_download.py::TestSafeFileCache::test_safe_set_no_perms", "tests/unit/test_download.py::Test_unpack_file_url::test_unpack_file_url_thats_a_dir", "tests/unit/test_download.py::TestPipSession::test_cache_is_enabled", "tests/unit/test_download.py::test_keyring_get_credential[http://example.com/path2/path3-expect1]", "tests/functional/test_install.py::test_basic_install_editable_from_hg", "tests/functional/test_install.py::test_install_argparse_shadowed", "tests/unit/test_download.py::test_user_agent__ci[BUILD_BUILDID-True]", "tests/unit/test_download.py::test_unpack_file_url_excludes_expected_dirs[.nox]", "tests/unit/test_download.py::test_url_to_path[file:///c:/tmp/file-C:\\\\tmp\\\\file-/c:/tmp/file]", "tests/unit/test_download.py::TestPipSession::test_cache_defaults_off", "tests/unit/test_download.py::test_keyring_get_password[http://user@example.com/path1-expect1]", "tests/unit/test_download.py::TestSafeFileCache::test_safe_get_no_perms", "tests/unit/test_download.py::test_sanitize_content_filename__platform_dependent[..\\\\..-..-..\\\\..]", "tests/unit/test_download.py::test_sanitize_content_filename[../-]", "tests/unit/test_download.py::TestSafeFileCache::test_cache_roundtrip", "tests/unit/test_download.py::TestPipSession::test_insecure_host_adapter", "tests/unit/test_download.py::test_get_credentials_parses_correctly[http://user%40email.com:password@example.com/path-http://example.com/path-user@email.com-password]", "tests/unit/test_download.py::Test_unpack_file_url::test_unpack_file_url_bad_hash", "tests/unit/test_utils_filesystem.py::test_is_socket[make_file-False]", "tests/unit/test_download.py::test_keyring_set_password[200-creds1-True]", "tests/unit/test_download.py::test__get_url_scheme[http://localhost:8080/-http]", "tests/unit/test_download.py::test_url_to_path[file:/path/to/file-\\\\path\\\\to\\\\file-/path/to/file]", "tests/functional/test_install.py::test_vcs_url_urlquote_normalization", "tests/unit/test_download.py::test_unpack_http_url_with_urllib_response_without_content_type", "tests/unit/test_download.py::test__get_url_scheme[file:c:/path/to/file-file]", "tests/unit/test_download.py::test_unpack_file_url_excludes_expected_dirs[.tox]", "tests/unit/test_download.py::TestPipSession::test_http_cache_is_not_enabled", "tests/unit/test_download.py::test_get_credentials_parses_correctly[http://token@example.com/path-http://example.com/path-token-]", "tests/unit/test_download.py::test_get_credentials_parses_correctly[http://username:password@example.com/path-http://example.com/path-username-password]", "tests/unit/test_download.py::test_url_to_path[file:///tmp/file-\\\\tmp\\\\file-/tmp/file]", "tests/unit/test_download.py::test_get_credentials_parses_correctly[http://example.com/path-http://example.com/path-None-None]", "tests/unit/test_download.py::test_url_to_path_path_to_url_symmetry_win", "tests/unit/test_download.py::test_unpack_http_url_bad_downloaded_checksum", "tests/unit/test_download.py::test_sanitize_content_filename__platform_dependent[..\\\\file-file-..\\\\file]", "tests/unit/test_download.py::test_user_agent__ci[CI-True]", "tests/functional/test_install.py::test_install_editable_from_bazaar", "tests/unit/test_download.py::Test_unpack_file_url::test_unpack_file_url_no_download", "tests/unit/test_download.py::test__get_url_scheme[-None]", "tests/unit/test_download.py::test_keyring_get_password[http://example.com/path1-expect0]", "tests/unit/test_download.py::test_user_agent", "tests/unit/test_download.py::test_sanitize_content_filename[../file-file]", "tests/unit/test_download.py::test_url_to_path[file:tmp-tmp-tmp]", "tests/unit/test_download.py::Test_unpack_file_url::test_unpack_file_url_and_download", "tests/functional/test_install.py::test_install_global_option_using_editable", "tests/unit/test_download.py::test_keyring_get_password[http://foo@example.com/path2/path3-expect4]", "tests/unit/test_utils_filesystem.py::test_is_socket[make_socket_file-True]", "tests/unit/test_download.py::test_keyring_get_password_after_prompt", "tests/unit/test_download.py::test_url_to_path[file://localhost/tmp/file-\\\\tmp\\\\file-/tmp/file]", "tests/unit/test_download.py::test_download_http_url__no_directory_traversal", "tests/unit/test_download.py::Test_unpack_file_url::test_unpack_file_url_download_already_exists", "tests/unit/test_download.py::test_user_agent__ci[BUILD-False]", "tests/unit/test_download.py::test_parse_content_disposition[attachment;filename=\"../file\"-df-file]", "tests/unit/test_download.py::test_url_to_path[file://somehost/tmp/file-\\\\\\\\somehost\\\\tmp\\\\file-None]", "tests/unit/test_download.py::test_keyring_set_password[403-creds0-False]", "tests/unit/test_download.py::test_user_agent_user_data", "tests/unit/test_utils_filesystem.py::test_is_socket[make_broken_symlink-False]", "tests/functional/test_install.py::test_vcs_url_final_slash_normalization", "tests/unit/test_download.py::test_keyring_get_credential[http://example.com/path1-expect0]", "tests/unit/test_download.py::test_copy_source_tree_with_socket", "tests/unit/test_download.py::Test_unpack_file_url::test_unpack_file_url_download_bad_hash", "tests/unit/test_download.py::test_sanitize_content_filename[../../file-file]", "tests/unit/test_download.py::test_sanitize_content_filename__platform_dependent[dir\\\\file-file-dir\\\\file]", "tests/unit/test_download.py::test_get_index_url_credentials", "tests/unit/test_utils_filesystem.py::test_is_socket[make_dir-False]", "tests/unit/test_download.py::test_copy_source_tree", "tests/unit/test_download.py::test_sanitize_content_filename[../..-..]", "tests/unit/test_download.py::test_keyring_set_password[200-creds2-False]", "tests/unit/test_download.py::test_get_credentials_uses_cached_credentials", "tests/unit/test_download.py::test_keyring_get_password_username_in_index", "tests/unit/test_download.py::test_keyring_get_password[http://example.com/path2/path3-expect3]", "tests/unit/test_download.py::TestSafeFileCache::test_safe_delete_no_perms", "tests/unit/test_download.py::test__get_url_scheme[file:/dev/null-file]", "tests/unit/test_download.py::test_keyring_get_credential[http://user2@example.com/path2/path3-expect2]", "tests/unit/test_download.py::test_user_agent__ci[PIP_IS_CI-True]", "tests/unit/test_download.py::test_keyring_get_password[http://user2@example.com/path1-expect2]"], "FAIL_TO_FAIL": ["tests/functional/test_install.py::test_pep518_uses_build_env[missing_setuptools-install]", "tests/functional/test_install.py::test_pep518_uses_build_env[missing_setuptools-wheel]", "tests/functional/test_install.py::test_pep518_uses_build_env[bad_setuptools-install]", "tests/functional/test_install.py::test_pep518_uses_build_env[bad_setuptools-wheel]", "tests/functional/test_install.py::test_pep518_build_env_uses_same_pip", "tests/functional/test_install.py::test_pep518_refuses_conflicting_requires", "tests/functional/test_install.py::test_pep518_refuses_invalid_requires", "tests/functional/test_install.py::test_pep518_refuses_invalid_build_system", "tests/functional/test_install.py::test_pep518_allows_missing_requires", "tests/functional/test_install.py::test_pep518_with_user_pip", "tests/functional/test_install.py::test_pep518_with_extra_and_markers", "tests/functional/test_install.py::test_pep518_with_namespace_package", "tests/functional/test_install.py::test_pep518_forkbombs[pep518_forkbomb-install]", "tests/functional/test_install.py::test_pep518_forkbombs[pep518_forkbomb-wheel]", "tests/functional/test_install.py::test_pep518_forkbombs[pep518_twin_forkbombs_first-install]", "tests/functional/test_install.py::test_pep518_forkbombs[pep518_twin_forkbombs_first-wheel]", "tests/functional/test_install.py::test_pep518_forkbombs[pep518_twin_forkbombs_second-install]", "tests/functional/test_install.py::test_pep518_forkbombs[pep518_twin_forkbombs_second-wheel]", "tests/functional/test_install.py::test_pip_second_command_line_interface_works", "tests/functional/test_install.py::test_install_exit_status_code_when_no_requirements", "tests/functional/test_install.py::test_install_exit_status_code_when_blank_requirements_file", "tests/functional/test_install.py::test_basic_install_from_pypi", "tests/functional/test_install.py::test_basic_editable_install", "tests/functional/test_install.py::test_basic_install_editable_from_svn", "tests/functional/test_install.py::test_basic_install_editable_from_git", "tests/functional/test_install.py::test_install_editable_from_git_autobuild_wheel", "tests/functional/test_install.py::test_install_editable_uninstalls_existing", "tests/functional/test_install.py::test_install_editable_uninstalls_existing_from_path", "tests/functional/test_install.py::test_basic_install_from_local_directory", "tests/functional/test_install.py::test_basic_install_relative_directory", "tests/functional/test_install.py::test_install_quiet", "tests/functional/test_install.py::test_hashed_install_success", "tests/functional/test_install.py::test_hashed_install_failure", "tests/functional/test_install.py::test_install_from_local_directory_with_symlinks_to_directories", "tests/functional/test_install.py::test_install_from_local_directory_with_socket_file", "tests/functional/test_install.py::test_install_from_local_directory_with_no_setup_py", "tests/functional/test_install.py::test_editable_install__local_dir_no_setup_py", "tests/functional/test_install.py::test_editable_install__local_dir_no_setup_py_with_pyproject", "tests/functional/test_install.py::test_upgrade_argparse_shadowed", "tests/functional/test_install.py::test_install_curdir", "tests/functional/test_install.py::test_install_pardir", "tests/functional/test_install.py::test_install_global_option", "tests/functional/test_install.py::test_install_with_hacked_egg_info", "tests/functional/test_install.py::test_install_using_install_option_and_editable", "tests/functional/test_install.py::test_install_package_with_same_name_in_curdir", "tests/functional/test_install.py::test_install_folder_using_dot_slash", "tests/functional/test_install.py::test_install_folder_using_slash_in_the_end", "tests/functional/test_install.py::test_install_folder_using_relative_path", "tests/functional/test_install.py::test_install_package_which_contains_dev_in_name", "tests/functional/test_install.py::test_install_package_with_target", "tests/functional/test_install.py::test_install_nonlocal_compatible_wheel", "tests/functional/test_install.py::test_install_nonlocal_compatible_wheel_path", "tests/functional/test_install.py::test_install_with_target_and_scripts_no_warning", "tests/functional/test_install.py::test_install_package_with_root", "tests/functional/test_install.py::test_install_package_with_prefix", "tests/functional/test_install.py::test_install_editable_with_prefix", "tests/functional/test_install.py::test_install_package_conflict_prefix_and_user", "tests/functional/test_install.py::test_install_package_that_emits_unicode", "tests/functional/test_install.py::test_install_package_with_utf8_setup", "tests/functional/test_install.py::test_install_package_with_latin1_setup", "tests/functional/test_install.py::test_url_req_case_mismatch_no_index", "tests/functional/test_install.py::test_url_req_case_mismatch_file_index", "tests/functional/test_install.py::test_url_incorrect_case_no_index", "tests/functional/test_install.py::test_url_incorrect_case_file_index", "tests/functional/test_install.py::test_compiles_pyc", "tests/functional/test_install.py::test_no_compiles_pyc", "tests/functional/test_install.py::test_install_upgrade_editable_depending_on_other_editable", "tests/functional/test_install.py::test_install_subprocess_output_handling", "tests/functional/test_install.py::test_install_log", "tests/functional/test_install.py::test_install_topological_sort", "tests/functional/test_install.py::test_install_wheel_broken", "tests/functional/test_install.py::test_cleanup_after_failed_wheel", "tests/functional/test_install.py::test_install_builds_wheels", "tests/functional/test_install.py::test_install_no_binary_disables_building_wheels", "tests/functional/test_install.py::test_install_no_binary_disables_cached_wheels", "tests/functional/test_install.py::test_install_editable_with_wrong_egg_name", "tests/functional/test_install.py::test_install_tar_xz", "tests/functional/test_install.py::test_install_tar_lzma", "tests/functional/test_install.py::test_double_install", "tests/functional/test_install.py::test_double_install_fail", "tests/functional/test_install.py::test_install_incompatible_python_requires", "tests/functional/test_install.py::test_install_incompatible_python_requires_editable", "tests/functional/test_install.py::test_install_incompatible_python_requires_wheel", "tests/functional/test_install.py::test_install_compatible_python_requires", "tests/functional/test_install.py::test_install_pep508_with_url", "tests/functional/test_install.py::test_install_pep508_with_url_in_install_requires", "tests/functional/test_install.py::test_install_from_test_pypi_with_ext_url_dep_is_blocked[https://pypi.org/simple]", "tests/functional/test_install.py::test_install_from_test_pypi_with_ext_url_dep_is_blocked[https://test.pypi.org/simple]", "tests/functional/test_install.py::test_installing_scripts_outside_path_prints_warning", "tests/functional/test_install.py::test_installing_scripts_outside_path_can_suppress_warning", "tests/functional/test_install.py::test_installing_scripts_on_path_does_not_print_warning", "tests/functional/test_install.py::test_installed_files_recorded_in_deterministic_order", "tests/functional/test_install.py::test_install_conflict_results_in_warning", "tests/functional/test_install.py::test_install_conflict_warning_can_be_suppressed", "tests/functional/test_install.py::test_target_install_ignores_distutils_config_install_prefix", "tests/unit/test_download.py::test_copy_source_tree_with_socket_fails_with_no_socket_error", "tests/unit/test_download.py::test_copy_source_tree_with_unreadable_dir_fails", "tests/unit/test_utils_filesystem.py::test_copy2_fixed_raises_appropriate_errors[make_unreadable_file-OSError]"], "difficulty_score": 3, "evaluation_score": 2, "image": "eval-pypa-pip-e554ef60", "spec_dict": {"python": "3.11", "pip_packages": ["cryptography", "mock", "pretend", "urllib3", "nose", "aiohappyeyeballs==2.6.1", "aiohttp==3.11.16", "aiohttp-jinja2==1.6", "aiopg==1.4.0", "aiosignal==1.3.2", "aiosocks==0.2.6", "async-timeout==4.0.3", "attrs==25.3.0", "distlib==0.3.9", "fake-useragent==2.1.0", "filelock==3.18.0", "freezegun==1.5.1", "frozenlist==1.5.0", "idna==3.10", "iniconfig==2.1.0", "installer==0.7.0", "Jinja2==3.1.6", "lxml==5.3.2", "MarkupSafe==3.0.2", "multidict==6.3.2", "packaging==24.2", "peewee==3.17.9", "peewee-async==1.1.0", "platformdirs==4.3.7", "pluggy==1.5.0", "propcache==0.3.1", "proxy==0.0.1", "proxy.py==2.4.10", "proxypy==2.0", "psycopg2-binary==2.9.10", "pyproxy==0.1.6", "PySocks==1.7.1", "pytest==8.3.5", "python-dateutil==2.9.0.post0", "scripttest==2.0", "six==1.17.0", "toml==0.10.2", "tomli==2.2.1", "tomli_w==1.2.0", "tornado==6.4.2", "typing_extensions==4.13.1", "virtualenv==20.30.0", "Werkzeug==3.1.3", "yarl==1.19.0"], "install": "export PATH=$PATH:$(pwd); pip install -e .", "test_cmd": "pytest --tb=short --json-report --json-report-file=/pass_report.json -W ignore::DeprecationWarning --continue-on-collection-errors", "pre_install": ""}, "task_score": 0, "version": "v1"}
{"repo": "encode/django-rest-framework", "pull_number": 6866, "instance_id": "encode__django-rest-framework-6866", "issue_numbers": ["6862"], "base_commit": "a1424675861e6add5a05710fc3a88922e529c340", "patch": "diff --git a/rest_framework/schemas/openapi.py b/rest_framework/schemas/openapi.py\n--- a/rest_framework/schemas/openapi.py\n+++ b/rest_framework/schemas/openapi.py\n@@ -378,7 +378,7 @@ def _map_serializer(self, serializer):\n                 schema['default'] = field.default\n             if field.help_text:\n                 schema['description'] = str(field.help_text)\n-            self._map_field_validators(field.validators, schema)\n+            self._map_field_validators(field, schema)\n \n             properties[field.field_name] = schema\n \n@@ -390,13 +390,11 @@ def _map_serializer(self, serializer):\n \n         return result\n \n-    def _map_field_validators(self, validators, schema):\n+    def _map_field_validators(self, field, schema):\n         \"\"\"\n         map field validators\n-        :param list:validators: list of field validators\n-        :param dict:schema: schema that the validators get added to\n         \"\"\"\n-        for v in validators:\n+        for v in field.validators:\n             # \"Formats such as \"email\", \"uuid\", and so on, MAY be used even though undefined by this specification.\"\n             # https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#data-types\n             if isinstance(v, EmailValidator):\n@@ -406,9 +404,15 @@ def _map_field_validators(self, validators, schema):\n             if isinstance(v, RegexValidator):\n                 schema['pattern'] = v.regex.pattern\n             elif isinstance(v, MaxLengthValidator):\n-                schema['maxLength'] = v.limit_value\n+                attr_name = 'maxLength'\n+                if isinstance(field, serializers.ListField):\n+                    attr_name = 'maxItems'\n+                schema[attr_name] = v.limit_value\n             elif isinstance(v, MinLengthValidator):\n-                schema['minLength'] = v.limit_value\n+                attr_name = 'minLength'\n+                if isinstance(field, serializers.ListField):\n+                    attr_name = 'minItems'\n+                schema[attr_name] = v.limit_value\n             elif isinstance(v, MaxValueValidator):\n                 schema['maximum'] = v.limit_value\n             elif isinstance(v, MinValueValidator):\n", "test_patch": "diff --git a/tests/schemas/test_openapi.py b/tests/schemas/test_openapi.py\n--- a/tests/schemas/test_openapi.py\n+++ b/tests/schemas/test_openapi.py\n@@ -395,6 +395,9 @@ def test_serializer_validators(self):\n         assert properties['string']['minLength'] == 2\n         assert properties['string']['maxLength'] == 10\n \n+        assert properties['lst']['minItems'] == 2\n+        assert properties['lst']['maxItems'] == 10\n+\n         assert properties['regex']['pattern'] == r'[ABC]12{3}'\n         assert properties['regex']['description'] == 'must have an A, B, or C followed by 1222'\n \ndiff --git a/tests/schemas/views.py b/tests/schemas/views.py\n--- a/tests/schemas/views.py\n+++ b/tests/schemas/views.py\n@@ -85,6 +85,12 @@ class ExampleValidatedSerializer(serializers.Serializer):\n         ),\n         help_text='must have an A, B, or C followed by 1222'\n     )\n+    lst = serializers.ListField(\n+        validators=(\n+            MaxLengthValidator(limit_value=10),\n+            MinLengthValidator(limit_value=2),\n+        )\n+    )\n     decimal1 = serializers.DecimalField(max_digits=6, decimal_places=2)\n     decimal2 = serializers.DecimalField(max_digits=5, decimal_places=0,\n                                         validators=(DecimalValidator(max_digits=17, decimal_places=4),))\n", "problem_statement": "DRF generates incorrect attributes for serializers.ListField\n## Checklist\r\n\r\n- [x] I have verified that that issue exists against the `master` branch of Django REST framework.\r\n- [x] I have searched for similar issues in both open and closed tickets and cannot find a duplicate.\r\n- [x] This is not a usage question. (Those should be directed to the [discussion group](https://groups.google.com/forum/#!forum/django-rest-framework) instead.)\r\n- [x] This cannot be dealt with as a third party library. (We prefer new functionality to be [in the form of third party libraries](https://www.django-rest-framework.org/community/third-party-packages/#about-third-party-packages) where possible.)\r\n- [x] I have reduced the issue to the simplest possible case.\r\n- [ ] I have included a failing test as a pull request. (If you are unable to do so we can still accept the issue.)\r\n\r\n## Steps to reproduce\r\n\r\n```\r\n# view.py\r\nfrom rest_framework import generics\r\nfrom rest_framework import serializers\r\nfrom django.db import models\r\n\r\nclass Book(models.Model):\r\n  title = models.CharField(max_length=200)\r\n\r\nclass BookSerializer(serializers.ModelSerializer):\r\n  tags = serializers.ListField(min_length=2, max_length=5)\r\n  class Meta:\r\n    model = Book\r\n    fields = ('tags',)\r\n\r\nclass BooksView(generics.ListAPIView):\r\n  queryset = Book.objects.all()   \r\n  serializer_class = BookSerializer\r\n\r\n# urls.py\r\nurlpatterns = [\r\n...\r\npath('api/books/', views.BooksView.as_view()),\r\n...\r\n]\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\n$ ./manage.py generateschema\r\n\r\nopenapi: 3.0.2\r\ninfo:\r\n  title: ''\r\n  version: TODO\r\npaths:\r\n  /api/books/:\r\n    get:\r\n      operationId: ListBooks\r\n      parameters: []\r\n      responses:\r\n        '200':\r\n          content:\r\n            application/json:\r\n              schema:\r\n                required:\r\n                - tags\r\n                properties:\r\n                  tags:\r\n                    type: array\r\n                    items: {}\r\n                    maxItems: 5\r\n                    minItems: 2\r\n```\r\n\r\n## Actual behavior\r\n\r\n```\r\n$ ./manage.py generateschema\r\n\r\nopenapi: 3.0.2\r\ninfo:\r\n  title: ''\r\n  version: TODO\r\npaths:\r\n  /api/books/:\r\n    get:\r\n      operationId: ListBooks\r\n      parameters: []\r\n      responses:\r\n        '200':\r\n          content:\r\n            application/json:\r\n              schema:\r\n                required:\r\n                - tags\r\n                properties:\r\n                  tags:\r\n                    type: array\r\n                    items: {}\r\n                    maxLength: 5\r\n                    minLength: 2\r\n```\r\n\r\nhttps://swagger.io/docs/specification/data-models/data-types/#array-length\n", "hints_text": "@carltongibson please confirm whether this is something you'd like me to work on \ud83d\ude42 \r\nThere's one more https://github.com/encode/django-rest-framework/issues/6863\nHi @knivets I\u2019m struggling to spot the difference between the actual and expected here. Could you ping it out for me please? (Sorry)\nOh, items vs length. Right. \nYes, I guess we need an adjustment here, after mapping validators. (Grrrr. Messy spec. Grrrr. \ud83d\ude42)", "created_at": "2019-08-11T13:06:27Z", "PASS_TO_PASS": ["tests/schemas/test_openapi.py::TestOperationIntrospection::test_response_body_nested_serializer", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_response_body_generation", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_delete_response_body_generation", "tests/schemas/test_openapi.py::TestGenerator::test_override_settings", "tests/schemas/test_openapi.py::TestGenerator::test_prefixed_paths_construction", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_empty_required", "tests/schemas/test_openapi.py::TestGenerator::test_schema_construction", "tests/schemas/test_openapi.py::TestGenerator::test_mount_url_prefixed_to_paths", "tests/schemas/test_openapi.py::TestBasics::test_pagination", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_path_with_id_parameter", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_list_response_body_generation", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_retrieve_response_body_generation", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_operation_id_generation", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_serializer_datefield", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_repeat_operation_ids", "tests/schemas/test_openapi.py::TestFieldMapping::test_list_field_mapping", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_path_without_parameters", "tests/schemas/test_openapi.py::TestBasics::test_filters", "tests/schemas/test_openapi.py::TestGenerator::test_paths_construction", "tests/schemas/test_openapi.py::TestOperationIntrospection::test_request_body", "tests/schemas/test_openapi.py::TestFieldMapping::test_lazy_string_field"], "FAIL_TO_PASS": ["tests/schemas/test_openapi.py::TestOperationIntrospection::test_serializer_validators"], "FAIL_TO_FAIL": [], "difficulty_score": 2, "evaluation_score": 2, "image": "eval-encode-django-rest-framework-a71e1a68", "spec_dict": {"python": "3.12.9", "install": "pip install -e .", "test_cmd": "pytest", "pre_install": [], "eval_commands": [], "additional_notes": "The tests ran successfully, but there were deprecation warnings related to Django and pytest. These warnings do not affect the test results and can be ignored for now.", "pip_packages": ["asgiref==3.8.1", "django==2.2", "iniconfig==2.0.0", "packaging==24.2", "pluggy==1.5.0", "pytest==8.3.5", "pytz==2025.1", "setuptools==76.0.0", "sqlparse==0.5.3", "uritemplate==4.1.1"]}, "task_score": 0}
